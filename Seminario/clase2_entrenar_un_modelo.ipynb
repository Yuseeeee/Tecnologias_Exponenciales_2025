{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a814376e",
   "metadata": {},
   "source": [
    "# Vamos a entrenar nuestro primer modelo!!!1!\n",
    "\n",
    "### 1. Cargamos nuestro dataset y distintas librerías que van a ser útiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc96da7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cancer_dataset = pd.read_csv('datos/cancer_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ba260d",
   "metadata": {},
   "source": [
    "### 2. Exploramos el dataset \n",
    "    \n",
    "Ejercicio: ¿qué columnas tiene? ¿qué tipo de datos hay en cada columna? Utilizar gráficos de la clase anterior.\n",
    "\n",
    "¿Cuales son las features y cual es la target? ¿Cuantos datos hay de cada clase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29884717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer ej"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e6aaa2",
   "metadata": {},
   "source": [
    "### 3. Separación en train y test:\n",
    "\n",
    "Si queremos evaluar qué tan bien funciona nuestro modelo, necesitamos separar una parte de los datos para no usarlos en el entrenamiento. De esta forma, podremos ver qué tan bien funciona el modelo con datos que no ha visto antes.\n",
    "\n",
    "Preguntas: ¿Podemos usar el 50% de los datos para testear? ¿Y el 1%? ¿Y el 90%?\n",
    "\n",
    "¿Se puede usar test para selección de hiperparámetros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834ec623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "\n",
    "X = cancer_dataset.drop(columns=['target'])\n",
    "y = cancer_dataset['target']\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=4, test_size=TEST_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a114ec",
   "metadata": {},
   "source": [
    "Podemos ver los tamaños de los nuevos conjuntos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4823837",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train dimensión: {X_train.shape}\")\n",
    "print(f\"y_train dimensión: {y_train.shape}\")\n",
    "\n",
    "print(f\"X_test dimensión: {X_test.shape}\")\n",
    "print(f\"y_test dimensión: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d042ac9",
   "metadata": {},
   "source": [
    "### 4. Construcción de un modelo \n",
    "\n",
    "Para construir nuestro árbol creamos un objeto de la clase [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) a la que más tarde cambiaremos parámetros.\n",
    "\n",
    "En este punto definimos que:\n",
    "  - la profundidad máxima del árbol será 1 (esta bien?)\n",
    "  - que el criterio para la selección en cada nodo sera `entropy`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc247125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "arbol = DecisionTreeClassifier(max_depth=1, criterion=\"entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75848f80",
   "metadata": {},
   "source": [
    "A entrenar el modelo!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b393f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "arbol.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd981d2f",
   "metadata": {},
   "source": [
    "### 5. Predicciones\n",
    "\n",
    "Imaginemos que tenemos una instancia que queremos predecir. Podemos hacer una predicción con el modelo que entrenamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71bc39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[17.99, 10.38, 122.8, 1001.0, 0.1184, 0.2776, 0.3001, 0.1471, 0.2419, 0.07871, 1.095, 0.9053, 8.589, 153.4, 0.006399, 0.04904, 0.05373, 0.01587, 0.03003, 0.006193, 25.38, 17.33, 184.6, 2019.0, 0.1622, 0.6656, 0.7119, 0.2654, 0.4601, 0.1189]])\n",
    "print(f\"X_new.shape: {X_new.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = arbol.predict(X_new)\n",
    "print(f\"Predicción: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d251ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predecimos los valores para las instacias que no vimos\n",
    "y_pred = arbol.predict(X_test)\n",
    "\n",
    "print(f\"Predicciones:   {y_pred}\\nValores reales: {y_test.to_numpy()}\")\n",
    "# Podemos calcular el accuracy (exactitud) comparando los valores predichos contra los reales, \n",
    "# para ello contamos cuántas coincidencias hubo y dividimos por la cantidad de comparaciones que hicimos:\n",
    "print(f\"Accuracy sobre el test set: {np.mean(y_pred == y_test)}\") \n",
    "\n",
    "#También podemos invocar al método score que viene con los DecisionTreeClassifier\n",
    "print(f\"Accuracy sobre el test set: {arbol.score(X_test, y_test)}\") \n",
    "\n",
    "print(f\"Score sobre el training set: {arbol.score(X_train, y_train):.2f} ¿qué indica este número?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f825b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Matriz de confusión:\")\n",
    "confusion = sklearn.metrics.confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "display(pd.DataFrame(confusion, columns=[\"0\", \"1\"], index=[\"0\", \"1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699d7855",
   "metadata": {},
   "source": [
    "### Resumen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bd91ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    X, y, random_state=4, test_size=TEST_SIZE)\n",
    "\n",
    "arbol = DecisionTreeClassifier(max_depth=1, criterion=\"entropy\")\n",
    "arbol.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Score sobre el training set: {arbol.score(X_train, y_train):.2f} ¿qué indica este número?\")\n",
    "print(f\"Score sobre el test set: {arbol.score(X_test, y_test):.2f}  ¿qué indica este número?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cf47c6",
   "metadata": {},
   "source": [
    "### Ejercicio: \n",
    "- Crear un notebook nuevo y repetir el proceso con otro dataset (puede ser el de estrellas o cualquier otro que les interese)\n",
    "- Pasos a seguir:\n",
    "    - Cargar el dataset\n",
    "    - Explorar el dataset\n",
    "    - Separar en train y test\n",
    "    - Entrenar un modelo (probar distintos hiperparámetros)\n",
    "    - Printear el score en train y en test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d926c6",
   "metadata": {},
   "source": [
    "## Extras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f478fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a plot of the accuracy vs max_depth\n",
    "max_depths = range(1, 50)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for max_depth in max_depths:\n",
    "    arbol = DecisionTreeClassifier(max_depth=max_depth, criterion=\"entropy\")\n",
    "    arbol.fit(X_train, y_train)\n",
    "    train_scores.append(arbol.score(X_train, y_train))\n",
    "    test_scores.append(arbol.score(X_test, y_test))\n",
    "plt.plot(max_depths, train_scores, label='Train Score')\n",
    "plt.plot(max_depths, test_scores, label='Test Score')\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016dee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pydotplus\n",
    "# También instalar Graphviz. (en ubuntu: sudo apt-get install graphviz)\n",
    "\n",
    "from six import StringIO  #pip3 install six\n",
    "##### from sklearn.externals.six import StringIO  # opción para versiones más viejas de sklearn\n",
    "from IPython.display import Image, display\n",
    "import pydotplus\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "breast_dataset = load_breast_cancer()\n",
    "\n",
    "    \n",
    "def dibujar_arbol(clf, c_name=breast_dataset.target_names, f_name=breast_dataset.feature_names):\n",
    "    #\n",
    "    # modo de uso: dibujar_arbol(arbol)\n",
    "    #\n",
    "    dot_data = StringIO()\n",
    "    sklearn.tree.export_graphviz(clf, out_file = dot_data,  \n",
    "                    filled = True, \n",
    "                    class_names = c_name,\n",
    "                    feature_names = f_name,\n",
    "                    special_characters = True)\n",
    "\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "    display(Image(graph.create_png()))\n",
    "\n",
    "\n",
    "def explore_decision_tree_boundaries(max_depth=None, criterion=\"entropy\", data_set=breast_dataset, cols=[0, 1]):\n",
    "    n_classes, plot_colors, plot_step = 2, \"ryb\", 0.02\n",
    "    fig=plt.figure(figsize=(10,7), dpi= 100, facecolor='w', edgecolor='k')\n",
    "    \n",
    "    X = data_set[\"data\"][:, cols]\n",
    "    y = data_set[\"target\"]\n",
    "\n",
    "    # Build and train Classifier\n",
    "    tree = DecisionTreeClassifier(max_depth=max_depth, criterion=criterion).fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        tree,\n",
    "        X,\n",
    "        cmap=plt.cm.RdYlBu,\n",
    "        response_method=\"predict\",\n",
    "        ax=ax,\n",
    "        xlabel=data_set.feature_names[cols[0]],\n",
    "        ylabel=data_set.feature_names[cols[1]],\n",
    "    )\n",
    "\n",
    "    # Plot the training points\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(\n",
    "            X[idx, 0],\n",
    "            X[idx, 1],\n",
    "            c=color,\n",
    "            label=data_set.target_names[i],\n",
    "            cmap=plt.cm.RdYlBu,\n",
    "            edgecolor=\"black\",\n",
    "            s=15,\n",
    "        )\n",
    "\n",
    "    plt.suptitle(f\"Fronteras de decisión de un árbol de altura {tree.get_depth()} y #hojas: {tree.get_n_leaves()}\")\n",
    "    plt.legend(loc=\"lower right\", borderpad=0, handletextpad=0)\n",
    "    _ = plt.axis(\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    dibujar_arbol(tree, f_name=[data_set.feature_names[cols[0]],data_set.feature_names[cols[1]]]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795bd8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_decision_tree_boundaries(max_depth=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
